experiment,display_name,checkpoint,architecture,tokenizer,total_params,loss,perplexity,accuracy,top5_accuracy,tokens_per_sec,eval_time_sec
#11,"Fine-tune v4 (Discrim. LR, 23M) * BEST",finetuned_shakespeare_v4.pt,6L/8H/512d/2048FFN,bpe,22977024,4.278658318423671,72.14358133118193,25.592660371032633,47.36603151542244,42411.473073021014,6.751380681991577
#13,Fine-tune v6 (Gradual Unfreezing),finetuned_shakespeare_v6.pt,6L/8H/512d/2048FFN,bpe,22977024,4.290229641543897,72.98322655621114,25.63317221725525,47.33669535091641,41978.13270795954,6.821075201034546
#12,Fine-tune v5 (Heavier Reg.),finetuned_shakespeare_v5.pt,6L/8H/512d/2048FFN,bpe,22977024,4.332288969079991,76.11831979529737,24.909546826106393,46.50096390254805,42168.54487066859,6.790274620056152
#10,"Pre-train v4 (324 books, 23M)",pretrained_gutenberg_v4.pt,6L/8H/512d/2048FFN,bpe,22977024,4.496445385963191,89.69772316468679,23.18255476084041,44.629386455073764,43120.36715977766,6.64038872718811
#8,"Fine-tune v2 (Discrim. LR, 7.3M)",finetuned_shakespeare_v2.pt,5L/6H/300d/1024FFN,bpe,7275300,4.5057494744010596,90.5361731717606,23.326832047685834,44.713505142589995,105691.23599525425,2.590489149093628
#7,Fine-tune v1 (Uniform LR),finetuned_shakespeare.pt,5L/6H/300d/1024FFN,bpe,7275300,4.581301828476245,97.64142388976528,22.526589527816736,43.56263148667602,105918.45968941126,2.5849318504333496
#9,"Pre-train v3 (324 books, 7.3M)",pretrained_gutenberg_v3.pt,5L/6H/300d/1024FFN,bpe,7275300,4.640526575652094,103.5988858736381,21.659518886902102,42.65303699150648,102997.65238191781,2.780024528503418
#5,BPE v4 (Scratch),best_model_bpe.pt,5L/6H/300d/1024FFN,bpe,6375300,4.734876476176425,113.84939473706002,20.25491133518776,40.861511937876685,116321.00991133318,2.3735694885253906
#6,AWD-LSTM Baseline,best_model_lstm.pt,LSTM/300d,lstm,20510800,4.898118686057043,134.037375949796,19.677575915623553,38.74413247566064,47283.266541724544,5.839190483093262
#8-pretrain,"Pre-train v2 (19 books, 30ep)",pretrained_gutenberg_v2.pt,5L/6H/300d/1024FFN,bpe,7275300,4.9692272620537725,143.91563531964778,18.668551309022906,39.481796400187,107685.08469501756,2.542524814605713
#7-pretrain,Pre-train v1 (19 books),pretrained_gutenberg.pt,5L/6H/300d/1024FFN,bpe,7275300,5.021267075692454,151.60327497197574,17.8843793828892,38.4419559373539,103603.11615548818,2.642700433731079
#1,Word-Level Baseline,best_model.pt,5L/6H/300d/1024FFN,word,8635520,5.046504299326188,155.47800883359008,18.635870993168684,39.2735155018392,39092.56058437038,6.230955362319946
