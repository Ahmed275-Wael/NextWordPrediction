[
  {
    "checkpoint": "best_model.pt",
    "display_name": "Word-Level Baseline",
    "experiment": "#1",
    "architecture": "5L/6H/300d/1024FFN",
    "tokenizer": "word",
    "total_params": 8635520,
    "trainable_params": 8635520,
    "loss": 5.046504299326188,
    "perplexity": 155.47800883359008,
    "accuracy": 18.635870993168684,
    "top5_accuracy": 39.2735155018392,
    "tokens_per_sec": 39092.56058437038,
    "num_batches": 60,
    "total_tokens": 243584,
    "eval_time_sec": 6.230955362319946
  },
  {
    "checkpoint": "best_model_bpe.pt",
    "display_name": "BPE v4 (Scratch)",
    "experiment": "#5",
    "architecture": "5L/6H/300d/1024FFN",
    "tokenizer": "bpe",
    "total_params": 6375300,
    "trainable_params": 6375300,
    "loss": 4.734876476176425,
    "perplexity": 113.84939473706002,
    "accuracy": 20.25491133518776,
    "top5_accuracy": 40.861511937876685,
    "tokens_per_sec": 116321.00991133318,
    "num_batches": 34,
    "total_tokens": 276096,
    "eval_time_sec": 2.3735694885253906
  },
  {
    "checkpoint": "best_model_lstm.pt",
    "display_name": "AWD-LSTM Baseline",
    "experiment": "#6",
    "architecture": "LSTM/300d",
    "tokenizer": "lstm",
    "total_params": 20510800,
    "trainable_params": 20510800,
    "loss": 4.898118686057043,
    "perplexity": 134.037375949796,
    "accuracy": 19.677575915623553,
    "top5_accuracy": 38.74413247566064,
    "tokens_per_sec": 47283.266541724544,
    "num_batches": 34,
    "total_tokens": 276096,
    "eval_time_sec": 5.839190483093262
  },
  {
    "checkpoint": "pretrained_gutenberg.pt",
    "display_name": "Pre-train v1 (19 books)",
    "experiment": "#7-pretrain",
    "architecture": "5L/6H/300d/1024FFN",
    "tokenizer": "bpe",
    "total_params": 7275300,
    "trainable_params": 7275300,
    "loss": 5.021267075692454,
    "perplexity": 151.60327497197574,
    "accuracy": 17.8843793828892,
    "top5_accuracy": 38.4419559373539,
    "tokens_per_sec": 103603.11615548818,
    "num_batches": 34,
    "total_tokens": 273792,
    "eval_time_sec": 2.642700433731079
  },
  {
    "checkpoint": "pretrained_gutenberg_v2.pt",
    "display_name": "Pre-train v2 (19 books, 30ep)",
    "experiment": "#8-pretrain",
    "architecture": "5L/6H/300d/1024FFN",
    "tokenizer": "bpe",
    "total_params": 7275300,
    "trainable_params": 7275300,
    "loss": 4.9692272620537725,
    "perplexity": 143.91563531964778,
    "accuracy": 18.668551309022906,
    "top5_accuracy": 39.481796400187,
    "tokens_per_sec": 107685.08469501756,
    "num_batches": 34,
    "total_tokens": 273792,
    "eval_time_sec": 2.542524814605713
  },
  {
    "checkpoint": "pretrained_gutenberg_v3.pt",
    "display_name": "Pre-train v3 (324 books, 7.3M)",
    "experiment": "#9",
    "architecture": "5L/6H/300d/1024FFN",
    "tokenizer": "bpe",
    "total_params": 7275300,
    "trainable_params": 7275300,
    "loss": 4.640526575652094,
    "perplexity": 103.5988858736381,
    "accuracy": 21.659518886902102,
    "top5_accuracy": 42.65303699150648,
    "tokens_per_sec": 102997.65238191781,
    "num_batches": 35,
    "total_tokens": 286336,
    "eval_time_sec": 2.780024528503418
  },
  {
    "checkpoint": "pretrained_gutenberg_v4.pt",
    "display_name": "Pre-train v4 (324 books, 23M)",
    "experiment": "#10",
    "architecture": "6L/8H/512d/2048FFN",
    "tokenizer": "bpe",
    "total_params": 22977024,
    "trainable_params": 22977024,
    "loss": 4.496445385963191,
    "perplexity": 89.69772316468679,
    "accuracy": 23.18255476084041,
    "top5_accuracy": 44.629386455073764,
    "tokens_per_sec": 43120.36715977766,
    "num_batches": 35,
    "total_tokens": 286336,
    "eval_time_sec": 6.64038872718811
  },
  {
    "checkpoint": "finetuned_shakespeare.pt",
    "display_name": "Fine-tune v1 (Uniform LR)",
    "experiment": "#7",
    "architecture": "5L/6H/300d/1024FFN",
    "tokenizer": "bpe",
    "total_params": 7275300,
    "trainable_params": 7275300,
    "loss": 4.581301828476245,
    "perplexity": 97.64142388976528,
    "accuracy": 22.526589527816736,
    "top5_accuracy": 43.56263148667602,
    "tokens_per_sec": 105918.45968941126,
    "num_batches": 34,
    "total_tokens": 273792,
    "eval_time_sec": 2.5849318504333496
  },
  {
    "checkpoint": "finetuned_shakespeare_v2.pt",
    "display_name": "Fine-tune v2 (Discrim. LR, 7.3M)",
    "experiment": "#8",
    "architecture": "5L/6H/300d/1024FFN",
    "tokenizer": "bpe",
    "total_params": 7275300,
    "trainable_params": 7275300,
    "loss": 4.5057494744010596,
    "perplexity": 90.5361731717606,
    "accuracy": 23.326832047685834,
    "top5_accuracy": 44.713505142589995,
    "tokens_per_sec": 105691.23599525425,
    "num_batches": 34,
    "total_tokens": 273792,
    "eval_time_sec": 2.590489149093628
  },
  {
    "checkpoint": "finetuned_shakespeare_v4.pt",
    "display_name": "Fine-tune v4 (Discrim. LR, 23M) * BEST",
    "experiment": "#11",
    "architecture": "6L/8H/512d/2048FFN",
    "tokenizer": "bpe",
    "total_params": 22977024,
    "trainable_params": 22977024,
    "loss": 4.278658318423671,
    "perplexity": 72.14358133118193,
    "accuracy": 25.592660371032633,
    "top5_accuracy": 47.36603151542244,
    "tokens_per_sec": 42411.473073021014,
    "num_batches": 35,
    "total_tokens": 286336,
    "eval_time_sec": 6.751380681991577
  },
  {
    "checkpoint": "finetuned_shakespeare_v5.pt",
    "display_name": "Fine-tune v5 (Heavier Reg.)",
    "experiment": "#12",
    "architecture": "6L/8H/512d/2048FFN",
    "tokenizer": "bpe",
    "total_params": 22977024,
    "trainable_params": 22977024,
    "loss": 4.332288969079991,
    "perplexity": 76.11831979529737,
    "accuracy": 24.909546826106393,
    "top5_accuracy": 46.50096390254805,
    "tokens_per_sec": 42168.54487066859,
    "num_batches": 35,
    "total_tokens": 286336,
    "eval_time_sec": 6.790274620056152
  },
  {
    "checkpoint": "finetuned_shakespeare_v6.pt",
    "display_name": "Fine-tune v6 (Gradual Unfreezing)",
    "experiment": "#13",
    "architecture": "6L/8H/512d/2048FFN",
    "tokenizer": "bpe",
    "total_params": 22977024,
    "trainable_params": 22977024,
    "loss": 4.290229641543897,
    "perplexity": 72.98322655621114,
    "accuracy": 25.63317221725525,
    "top5_accuracy": 47.33669535091641,
    "tokens_per_sec": 41978.13270795954,
    "num_batches": 35,
    "total_tokens": 286336,
    "eval_time_sec": 6.821075201034546
  }
]